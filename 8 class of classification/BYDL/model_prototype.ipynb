{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://localhost:9000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [29/Sep/2021 23:14:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Sep/2021 23:14:55] \"\u001b[33mGET /static/css/styles.css HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [29/Sep/2021 23:15:13] \"\u001b[37mPOST /predict HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [29/Sep/2021 23:15:13] \"\u001b[33mGET /static/css/styles.css HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask,render_template,url_for,request\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "#from sklearn.externals import joblib\n",
    "#from joblib import joblib\n",
    "from joblib import Memory\n",
    "cachedir = 'your_cache_dir_goes_here'\n",
    "mem = Memory(cachedir)\n",
    "from joblib import Parallel, delayed\n",
    "from flask import Flask\n",
    "from gevent.pywsgi import WSGIServer\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.config['TESTING'] = True\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "\treturn render_template('home.html')\n",
    "\n",
    "@app.route('/predict',methods=['POST'])\n",
    "def predict():\n",
    "    xl = pd.ExcelFile(\"final_datasets8.xlsx\")\n",
    "    df = xl.parse( header=None, names=['ID','Level','Date','Label','Msg'], encoding='utf-8')\n",
    "    X = df['Msg']\n",
    "    y = df['Label']\n",
    "    from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "    Tokenizer_obj=Tokenizer()\n",
    "    Tokenizer_obj.fit_on_texts(X)\n",
    "    max_length=48\n",
    "    vocab_size=len(Tokenizer_obj.word_index)+1\n",
    "\t# Extract Feature With CountVectorizer\n",
    "\t#cv = CountVectorizer()\n",
    "\t#X = cv.fit_transform(X) # Fit the Data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    x_train_tokens=Tokenizer_obj.texts_to_sequences(x_train)\n",
    "    x_test_tokens=Tokenizer_obj.texts_to_sequences(x_test)\n",
    "    x_train_pad=pad_sequences(x_train_tokens,maxlen=max_length,padding='post')\n",
    "    x_test_pad=pad_sequences(x_test_tokens,maxlen=max_length,padding='post')\n",
    "    # load json and create model\n",
    "    # load json and create model\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open('model folder/pretbilstmmodel.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model folder/pretbilstmmodel.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "    if request.method == 'POST':\n",
    "        message = request.form['message']\n",
    "        data = [message]\n",
    "        #import re\n",
    "        #from nltk.tokenize import word_tokenize\n",
    "        #word=word_tokenize(data)\n",
    "    #data= data.str.replace('[^\\w\\s]','')\n",
    "    #word = word.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    from textblob import TextBlob\n",
    "\n",
    "    def no_user_alpha(data):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        read=open(\"stopword.txt\",encoding='utf-8',mode='r')\n",
    "        readfirst=read.read()\n",
    "        clean_mess = [word for word in clean_s.split() if word not in readfirst]\n",
    "        return clean_mess\n",
    "    def Abrevation(data):\n",
    "        normalized_tweet = []\n",
    "        for word in data:\n",
    "            text=re.sub(\"ት/ቤት\",\"ትምህርት ቤት\",data)\n",
    "            text=re.sub(\"ት/ክፍል\",\"ትምህርት ክፍል\",data)\n",
    "            text=re.sub(\"ሃ/አለቃ\",\"ሀምሳ አለቃ\",data)\n",
    "            text=re.sub(\"ሃ/ስላሴ\",\"ሀይለ ስላሴ\",data)\n",
    "            text=re.sub(\"ደ/ዘይት\",\"ደብረ ዘይት\",data)\n",
    "            text=re.sub(\"ደ/ታቦር\",\"ደብረ ታቦር\",data)\n",
    "            text=re.sub(\"መ/ር\",\"መምህር\",data)\n",
    "            text=re.sub(\"መ/ቤት\",\"መስሪያ ቤት\",data)\n",
    "            text=re.sub(\"መ/አለቃ\",\" መቶ አለቃ\",data)\n",
    "            text=re.sub(\"ክ/ከተማ\",\"ክፍለ ከተማ\",data)\n",
    "            text=re.sub(\"ክ/ሀገር\",\"ክፍለ ሀገር\",data)\n",
    "            text=re.sub(\"ወ/ር\",\"ወታደር\",data)\n",
    "            text=re.sub(\"ወ/ሮ\",\"ወይዘሮ\",data)\n",
    "            text=re.sub(\"ወ/ሪት\",\"ወይዘሪት\",data)\n",
    "            text=re.sub(\"ዲ/ን\",\"ዲያቆን\",data)\n",
    "            text=re.sub(\"ወ/ስላሴ\",\"ወለተ ስላሴ\",data)\n",
    "            text=re.sub(\"ፍ/ስላሴ\",\"ፍቅረ ስላሴ\",data)\n",
    "            text=re.sub(\"ፍ/ቤት\",\"ፍርድ ቤት\",data)\n",
    "            text=re.sub(\"ጽ/ቤት \",\"ትምህርት ቤት\",data)\n",
    "            text=re.sub(\"ት/ሚኒስተር\",\"ትምህርት ሚኒስተር\",data)\n",
    "            text=re.sub(\"ት/ቤት\",\"ትምህርት ቤት\",data)\n",
    "            text=re.sub(\"10\",\"አስር\",data)\n",
    "            text=re.sub(\"ት/ቤት\",\"ጽህፈት ቤት\",data)\n",
    "            text=re.sub(\"ሲ/ር\",\"ሲስተር\",data)\n",
    "            text=re.sub(\"ጠ/ሚኒስትር\",\"ጠቅላይ ሚኒስትር\",data)\n",
    "            text=re.sub(\"ዶ/ር\",\"ዶክተር\",data)\n",
    "            text=re.sub(\"ገ/ጊዮርጊስ\",\"ገብረ ጊዮርጊስ\",data)\n",
    "            text=re.sub(\"ቤ/ክርስቲያን\",\"ቤተ ክርስቲያን\",data)\n",
    "            text=re.sub(\"ም/ሊቀመንበር\",\"ምክትል ሊቀመንበር\",data)\n",
    "            text=re.sub(\"ም/ቤት\",\"ምክር ቤት\",data)\n",
    "            text=re.sub(\"ተ/ሃይማኖት\",\"ተክለ ሃይማኖት\",data)\n",
    "            text=re.sub(\"ገ/ፃድቅ\",\"ገብረ ፃድቅ\",data)\n",
    "            text=re.sub(\"ሚ/ር\",\"ሚኒስትር\",data)\n",
    "            text=re.sub(\"ኮ/ል\",\"ኮለኔል\",data)\n",
    "            text=re.sub(\"ሜ/ጀነራል\",\"ሜጀር ጀነራል\",data)\n",
    "            text=re.sub(\"ብ/ጀነራል\",\"ብርጋዴን ጀነራል\",data)\n",
    "            text=re.sub(\"ሌ/ኮለኔል\",\"ሌተናል ኮለኔል\",data)\n",
    "            text=re.sub(\"ሊ/መንበር\",\"ሊቀ መንበር\",data)\n",
    "            text=re.sub(\"ር/መምህር \",\"ርእሰ መምህር\",data)\n",
    "            text=re.sub(\"ፕ/ት\",\"ፕሬዘዳንት\",data)\n",
    "            text=re.sub(\"ዓ.ም\",\"አመተ ምህረት\",data)\n",
    "            text=re.sub(\"ዓ/ም\",\"አመተ ምህረት\",data)\n",
    "            text=re.sub(\"አ/አ\",\"አዲስ አበባ\",data)\n",
    "            text=re.sub(\"ገ/ጊዮርጊስ\",\"ገብረ ጊዮርጊስ\",data)\n",
    "            text=re.sub(\"ዓ.ዓ\",\"አዲስ አበባ\",data)\n",
    "            text=re.sub(\"ዶ.ር\",\"ዶክተር\",data)\n",
    "            text=re.sub(\"ፕ/ር\",\"ፕሮፌሰር\",data)\n",
    "            text=re.sub(\"1\",\"አንድ \",data)\n",
    "            text=re.sub(\"2\",\"ሁለት\",data)\n",
    "            text=re.sub(\"3\",\"ሶስት\",data)\n",
    "            text=re.sub(\"4\",\"አራት\",data)\n",
    "            text=re.sub(\"5\",\"አምስት \",data)\n",
    "            text=re.sub(\"6\",\"ስድስት\",data)\n",
    "            text=re.sub(\"7\",\"ሰባት\",data)\n",
    "            text=re.sub(\"8\",\"ስምንት \",data)\n",
    "            text=re.sub(\"9\",\"ዘጠኝ\",data)\n",
    "            text=re.sub(\"10\",\"አስር\",data)\n",
    "            text=re.sub(\"11\",\"አስራ አንድ\",data)\n",
    "            text=re.sub(\"12\",\"አስራ ሁለት\",data)\n",
    "            text=re.sub(\"13\",\"አስራ ሶስት\",data)\n",
    "            text=re.sub(\"14\",\"አስራ አራት\",data)\n",
    "            text=re.sub(\"15\",\"አስራ አምስት\",data)\n",
    "            text=re.sub(\"16\",\"አስራ ስድስት\",data)\n",
    "            text=re.sub(\"17\",\"አስራ ሰባት\",data)\n",
    "            text=re.sub(\"18\",\"አስራ ስምንት\",data)\n",
    "            text=re.sub(\"19\",\"አስራ ዘጠኝ\",data)\n",
    "            text=re.sub(\"20\",\"ሃያ\",data)\n",
    "            text=re.sub(\"30\",\"ሰላሳ\",data)\n",
    "            text=re.sub(\"40\",\"አርባ\",data)\n",
    "            text=re.sub(\"50\",\"ኃምሳ\",data)\n",
    "            text=re.sub(\"60\",\"ስልሳ\",data)\n",
    "            text=re.sub(\"70\",\"ሰባ\",data)\n",
    "            text=re.sub(\"80\",\"ሰማንያ\",data)\n",
    "            text=re.sub(\"90\",\"ዘጠና\",data)\n",
    "            text=re.sub(\"100\",\"መቶ\",data)\n",
    "            text=re.sub(\"200\",\"ሁለት መቶ\",data)\n",
    "            text=re.sub(\"300\",\"ሶስት መቶ\",data)\n",
    "            text=re.sub(\"400\",\"አራት መቶ\",data)\n",
    "            text=re.sub(\"500\",\"አምስት መቶ\",data)\n",
    "            text=re.sub(\"600\",\"ስድስት መቶ\",data)\n",
    "            text=re.sub(\"700\",\"ሰባት መቶ\",data)\n",
    "            text=re.sub(\"800\",\"ስምንት መቶ\",data)\n",
    "            text=re.sub(\"900\",\"ዘጠኝ መቶ\",data)\n",
    "            text=re.sub(\"1000\",\"አንድ ሺህ \",data)\n",
    "            text=re.sub(\"2000\",\"ሁለት ሺህ\",data)\n",
    "            text=re.sub(\"3000\",\"ሶስት ሺህ\",data)\n",
    "            text=re.sub(\"4000\",\"አራት ሺህ\",data)\n",
    "            text=re.sub(\"5000\",\"አምስት ሺህ \",data)\n",
    "            text=re.sub(\"6000\",\"ስድስት ሺህ\",data)\n",
    "            text=re.sub(\"7000\",\"ሰባት ሺህ\",data)\n",
    "            text=re.sub(\"8000\",\"ስምንት ሺህ \",data)\n",
    "            text=re.sub(\"9000\",\"ዘጠኝ ሺህ\",data)\n",
    "            text=re.sub(\"1000000\",\"አንድ ሚሊዮን\",data)\n",
    "        normalized_tweet.append(text)\n",
    "        return normalized_tweet\n",
    "\n",
    "    def normalization(data):\n",
    "        #lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in data:\n",
    "            normalized_text =re.sub(\"['ሐ','ኀ','ኃ','ሃ','ሓ','ኻ']\",\"ሀ\",data)\n",
    "            normalized_text=re.sub(\"['ሐ','ኀ','ኃ','ሃ','ሓ','ኻ']\",\"ሀ\",data)\n",
    "            normalized_text=re.sub(\"['ሑ','ኁ']\",\"ሁ\",data)\n",
    "            normalized_text=re.sub(\"['ሒ','ኂ']\",\"ሂ\",data)\n",
    "            normalized_text=re.sub(\"['ሔ','ኄ']\",\"ሄ\",data)\n",
    "            normalized_text=re.sub(\"['ሕ','ኅ']\",\"ህ\",data)\n",
    "            normalized_text=re.sub(\"['ሖ','ኆ']\",\"ሆ\",data)\n",
    "            normalized_text=re.sub(\"['A','ዓ','ኣ']\",\"A\",data)\n",
    "            normalized_text=re.sub(\"['U']\",\"U\",data)\n",
    "            normalized_text=re.sub(\"['I']\",\"I\",data)\n",
    "            normalized_text=re.sub(\"['ዔ']\",\"ኤ\",data)\n",
    "            normalized_text=re.sub(\"['E']\",\"E\",data)\n",
    "            normalized_text=re.sub(\"['O']\",\"O\",data)\n",
    "            normalized_text=re.sub(\"['ሠ']\",\"ሰ\",data)\n",
    "            normalized_text=re.sub(\"['ሡ']\",\"ሱ\",data)\n",
    "            normalized_text=re.sub(\"['ሢ']\",\"ሲ\",data)\n",
    "            normalized_text=re.sub(\"['ሣ']\",\"ሳ\",data)\n",
    "            normalized_text=re.sub(\"['ሤ']\",\"ሴ\",data)\n",
    "            normalized_text=re.sub(\"['ሥ']\",\"ስ\",data)\n",
    "            normalized_text=re.sub(\"['ሦ']\",\"ሶ\",data)\n",
    "            normalized_text=re.sub(\"['ፀ']\",\"ጸ\",data)\n",
    "            normalized_text=re.sub(\"['ፁ']\",\"ጹ\",data)\n",
    "            normalized_text=re.sub(\"['ፂ']\",\"ጺ\",data)\n",
    "            normalized_text=re.sub(\"['ፃ']\",\"ጻ\",data)\n",
    "            normalized_text=re.sub(\"['ፄ']\",\"ጼ\",data)\n",
    "            normalized_text=re.sub(\"['ፅ']\",\"ጽ\",data)\n",
    "            normalized_text=re.sub(\"['ፆ']\",\"ጾ\",data)\n",
    "            normalized_text=re.sub(\"['ዩ']\",\"የ\",data)\n",
    "            normalized_text=re.sub(\"['ዉ']\",\"ው\",data)\n",
    "            normalized_text=re.findall(r\"\\d+\",data)\n",
    "\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    " \n",
    "    tests_tokens=Tokenizer_obj.texts_to_sequences(data)\n",
    "\n",
    "    tests_tokens_pad=pad_sequences(tests_tokens,maxlen=max_length)\n",
    "    predicted=loaded_model.predict(tests_tokens_pad)\n",
    "    my_prediction = np.argmax(predicted, axis=1)\n",
    "    return render_template('result.html',prediction = my_prediction)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from werkzeug.serving import run_simple\n",
    "    run_simple('localhost', 9000, app)\n",
    "\n",
    "    #app.debug = True\n",
    "    #app.run()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "twt1 = [ \"durba gabaabduu hin fuudhinaa\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing \n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 1000000 # this means 15000 unique words can be taken \n",
    "tokenizer=Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(twt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model folder/pretbilstmmodel.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model folder/pretbilstmmodel.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "1/1 [==============================] - 1s 938ms/step\n",
      "Race Related Free\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load and predict from saved place \n",
    "\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt2 = tokenizer.texts_to_sequences(twt1)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt3 = pad_sequences(twt2, padding='post', maxlen=48, dtype='int64', value=0)\n",
    "print(twt3)\n",
    "sentiment = loaded_model.predict(twt3,batch_size=1,verbose = 1)[0]\n",
    "if(np.argmax(sentiment) == 1):\n",
    "    print(\"Gender reated hate \")\n",
    "elif (np.argmax(sentiment) == 10):\n",
    "    print(\"Gender Related Free\")\n",
    "elif (np.argmax(sentiment) == 2):\n",
    "    print(\"Religion Related Hate\")\n",
    "elif (np.argmax(sentiment) == 20):\n",
    "    print(\"Religion Related Free\")\n",
    "elif (np.argmax(sentiment) == 3):\n",
    "    print(\"Race Related Hate\")\n",
    "elif (np.argmax(sentiment) == 30):\n",
    "    print(\"Race Related Free\")\n",
    "elif (np.argmax(sentiment) == 4):\n",
    "    print(\"Offensive Related Hate\")\n",
    "elif (np.argmax(sentiment) == 40):\n",
    "    print(\"Offensive Related Not Hate\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
